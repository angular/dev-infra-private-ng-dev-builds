{"version":3,"file":"migrate.js","sourceRoot":"","sources":["migrate.ts"],"names":[],"mappings":"AAQA,OAAO,EAAC,WAAW,EAAC,MAAM,eAAe,CAAC;AAE1C,OAAO,EAAC,QAAQ,EAAE,SAAS,EAAC,MAAM,aAAa,CAAC;AAChD,OAAO,EAAC,SAAS,EAAE,OAAO,EAAC,MAAM,cAAc,CAAC;AAChD,OAAO,EAAC,aAAa,EAAE,mBAAmB,EAAE,eAAe,EAAC,MAAM,aAAa,CAAC;AAChF,OAAO,MAAM,MAAM,aAAa,CAAC;AACjC,OAAO,EAAC,GAAG,EAAC,MAAM,qBAAqB,CAAC;AACxC,OAAO,IAAI,MAAM,WAAW,CAAC;AAwB7B,SAAS,OAAO,CAAC,IAAU;IACzB,OAAO,IAAI;SACR,MAAM,CAAC,QAAQ,EAAE;QAChB,IAAI,EAAE,QAAQ;QACd,KAAK,EAAE,GAAG;QACV,WAAW,EAAE,uDAAuD;QACpE,YAAY,EAAE,IAAI;KACnB,CAAC;SACD,MAAM,CAAC,OAAO,EAAE;QACf,IAAI,EAAE,QAAQ;QACd,KAAK,EAAE,GAAG;QACV,WAAW,EAAE,4CAA4C;QACzD,YAAY,EAAE,IAAI;KACnB,CAAC;SACD,MAAM,CAAC,OAAO,EAAE;QACf,IAAI,EAAE,QAAQ;QACd,KAAK,EAAE,GAAG;QACV,WAAW,EAAE,gCAAgC;QAC7C,OAAO,EAAE,aAAa;KACvB,CAAC;SACD,MAAM,CAAC,gBAAgB,EAAE;QACxB,IAAI,EAAE,QAAQ;QACd,OAAO,EAAE,EAAE;QACX,WAAW,EACT,wFAAwF;KAC3F,CAAC;SACD,MAAM,CAAC,aAAa,EAAE;QACrB,IAAI,EAAE,QAAQ;QACd,KAAK,EAAE,GAAG;QACV,OAAO,EAAE,mBAAmB;QAC5B,WAAW,EAAE,4EAA4E;KAC1F,CAAC;SACD,MAAM,CAAC,QAAQ,EAAE;QAChB,IAAI,EAAE,QAAQ;QACd,KAAK,EAAE,GAAG;QACV,WAAW,EAAE,kDAAkD;KAChE,CAAC,CAAC;AACP,CAAC;AAGD,KAAK,UAAU,OAAO,CAAC,OAA2B;IAChD,MAAM,MAAM,GAAG,OAAO,CAAC,MAAM,IAAI,eAAe,CAAC;IAEjD,MAAM,CACJ,MAAM,EACN;QACE,0FAA0F;YACxF,oDAAoD;QACtD,4CAA4C;KAC7C,CAAC,IAAI,CAAC,IAAI,CAAC,CACb,CAAC;IAEF,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC;QACxC,IAAI,CAAC,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC;QACrB,QAAQ,CAAC,OAAO,CAAC,MAAM,EAAE,OAAO,CAAC;KAClC,CAAC,CAAC;IAEH,IAAI,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE,CAAC;QACvB,GAAG,CAAC,KAAK,CAAC,iCAAiC,OAAO,CAAC,KAAK,GAAG,CAAC,CAAC;QAC7D,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;IAClB,CAAC;IAED,MAAM,EAAE,GAAG,IAAI,WAAW,CAAC,EAAC,MAAM,EAAC,CAAC,CAAC;IACrC,MAAM,WAAW,GAAG,IAAI,SAAS,CAAC,EAAE,EAAE,OAAO,CAAC,WAAW,CAAC,CAAC;IAC3D,MAAM,QAAQ,GAAoC,EAAE,CAAC;IACrD,MAAM,OAAO,GAAG,IAAI,GAAG,EAAiB,CAAC;IAEzC,GAAG,CAAC,IAAI,CACN;QACE,wBAAwB,OAAO,CAAC,MAAM,OAAO,KAAK,CAAC,MAAM,YAAY;QACrE,eAAe,OAAO,CAAC,KAAK,0BAA0B,OAAO,CAAC,WAAW,GAAG;QAC5E,EAAE;KACH,CAAC,IAAI,CAAC,IAAI,CAAC,CACb,CAAC;IACF,WAAW,CAAC,KAAK,CAAC,KAAK,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;IAKnC,OAAO,KAAK,CAAC,MAAM,GAAG,CAAC,IAAI,OAAO,CAAC,IAAI,GAAG,CAAC,EAAE,CAAC;QAE5C,OAAO,KAAK,CAAC,MAAM,GAAG,CAAC,IAAI,OAAO,CAAC,IAAI,GAAG,OAAO,CAAC,cAAc,EAAE,CAAC;YACjE,MAAM,IAAI,GAAG,KAAK,CAAC,KAAK,EAAG,CAAC;YAC5B,MAAM,IAAI,GAAG,WAAW,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,OAAO,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC;YACnE,OAAO,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;QACpB,CAAC;QAGD,IAAI,OAAO,CAAC,IAAI,GAAG,CAAC,EAAE,CAAC;YACrB,MAAM,OAAO,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;QAC9B,CAAC;IACH,CAAC;IAED,WAAW,CAAC,IAAI,EAAE,CAAC;IAEnB,KAAK,MAAM,EAAC,IAAI,EAAE,KAAK,EAAC,IAAI,QAAQ,EAAE,CAAC;QACrC,GAAG,CAAC,IAAI,CAAC,uCAAuC,CAAC,CAAC;QAClD,GAAG,CAAC,IAAI,CAAC,GAAG,IAAI,qBAAqB,CAAC,CAAC;QACvC,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;IAClB,CAAC;IAED,GAAG,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;IAEtB,IAAI,QAAQ,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;QACxB,GAAG,CAAC,IAAI,CAAC,GAAG,QAAQ,CAAC,MAAM,uDAAuD,CAAC,CAAC;IACtF,CAAC;IAED,KAAK,UAAU,WAAW,CAAC,IAAY;QACrC,IAAI,CAAC;YACH,MAAM,OAAO,GAAG,MAAM,QAAQ,CAAC,IAAI,EAAE,OAAO,CAAC,CAAC;YAC9C,MAAM,MAAM,GAAG,MAAM,WAAW,CAAC,EAAE,EAAE,OAAO,CAAC,KAAK,EAAE,OAAO,CAAC,WAAW,EAAE,OAAO,EAAE,MAAM,CAAC,CAAC;YAC1F,MAAM,SAAS,CAAC,IAAI,EAAE,MAAM,CAAC,CAAC;QAChC,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC;YACX,QAAQ,CAAC,IAAI,CAAC,EAAC,IAAI,EAAE,IAAI,EAAE,KAAK,EAAG,CAAW,CAAC,QAAQ,EAAE,EAAC,CAAC,CAAC;QAC9D,CAAC;gBAAS,CAAC;YACT,WAAW,CAAC,SAAS,EAAE,CAAC;QAC1B,CAAC;IACH,CAAC;AACH,CAAC;AAUD,KAAK,UAAU,WAAW,CACxB,EAAe,EACf,KAAa,EACb,WAAmB,EACnB,OAAe,EACf,MAAc;IAGd,MAAM,cAAc,GAAG;QACrB,IAAI,EAAE,QAAQ;QACd,UAAU,EAAE;YACV,OAAO,EAAE,EAAC,IAAI,EAAE,QAAQ,EAAE,WAAW,EAAE,6BAA6B,EAAC;SACtE;QACD,QAAQ,EAAE,CAAC,SAAS,CAAC;QACrB,oBAAoB,EAAE,KAAK;QAC3B,OAAO,EAAE,yCAAyC;KACnD,CAAC;IAKF,MAAM,QAAQ,GAAG,MAAM,EAAE,CAAC,MAAM,CAAC,eAAe,CAAC;QAC/C,KAAK;QACL,QAAQ,EAAE,CAAC,EAAC,IAAI,EAAE,MAAM,EAAC,EAAE,EAAC,IAAI,EAAE,OAAO,EAAC,CAAC;QAC3C,MAAM,EAAE;YACN,gBAAgB,EAAE,kBAAkB;YACpC,cAAc;YACd,WAAW;YAEX,eAAe,EAAE,QAAQ;YAEzB,cAAc,EAAE,CAAC;YAEjB,iBAAiB,EACf,wDAAwD;gBACxD,mEAAmE;SACtE;KACF,CAAC,CAAC;IAEH,MAAM,IAAI,GAAG,QAAQ,CAAC,IAAI,CAAC;IAE3B,IAAI,CAAC,IAAI,EAAE,CAAC;QACV,MAAM,IAAI,KAAK,CAAC,uCAAuC,GAAG,IAAI,CAAC,SAAS,CAAC,QAAQ,EAAE,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC;IAC/F,CAAC;IAED,IAAI,MAA0B,CAAC;IAE/B,IAAI,CAAC;QACH,MAAM,GAAG,IAAI,CAAC,KAAK,CAAC,IAAI,CAAsB,CAAC;IACjD,CAAC;IAAC,MAAM,CAAC;QACP,MAAM,IAAI,KAAK,CACb,uEAAuE;YACrE,4DAA4D;YAC5D,aAAa;YACb,IAAI,CAAC,SAAS,CAAC,QAAQ,EAAE,IAAI,EAAE,CAAC,CAAC,CACpC,CAAC;IACJ,CAAC;IAED,IAAI,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC;QACpB,MAAM,IAAI,KAAK,CACb,6EAA6E;YAC3E,iDAAiD;YACjD,IAAI,CAAC,SAAS,CAAC,MAAM,EAAE,IAAI,EAAE,CAAC,CAAC,CAClC,CAAC;IACJ,CAAC;IAED,OAAO,MAAM,CAAC,OAAO,CAAC;AACxB,CAAC;AAGD,MAAM,CAAC,MAAM,aAAa,GAA+B;IACvD,OAAO;IACP,OAAO;IACP,OAAO,EAAE,SAAS;IAClB,QAAQ,EAAE,uDAAuD;CAClE,CAAC","sourcesContent":["/**\n * @license\n * Copyright Google LLC\n *\n * Use of this source code is governed by an MIT-style license that can be\n * found in the LICENSE file at https://angular.io/license\n */\n\nimport {GoogleGenAI} from '@google/genai';\nimport {Argv, Arguments, CommandModule} from 'yargs';\nimport {readFile, writeFile} from 'fs/promises';\nimport {SingleBar, Presets} from 'cli-progress';\nimport {DEFAULT_MODEL, DEFAULT_TEMPERATURE, DEFAULT_API_KEY} from './consts.js';\nimport assert from 'node:assert';\nimport {Log} from '../utils/logging.js';\nimport glob from 'fast-glob';\n\n/** Command line options. */\nexport interface Options {\n  /** Prompt that should be applied. */\n  prompt: string;\n\n  /** Glob of files that the prompt should apply to. */\n  files: string;\n\n  /** Model that should be used to apply the prompt. */\n  model: string;\n\n  /** Temperature for the model. */\n  temperature: number;\n\n  /** Maximum number of concurrent API requests. */\n  maxConcurrency: number;\n\n  /** API key to use when making requests. */\n  apiKey?: string;\n}\n\n/** Yargs command builder for the command. */\nfunction builder(argv: Argv): Argv<Options> {\n  return argv\n    .option('prompt', {\n      type: 'string',\n      alias: 'p',\n      description: 'Path to the file containg the prompt that will be run',\n      demandOption: true,\n    })\n    .option('files', {\n      type: 'string',\n      alias: 'f',\n      description: 'Glob for the files that should be migrated',\n      demandOption: true,\n    })\n    .option('model', {\n      type: 'string',\n      alias: 'm',\n      description: 'Model to use for the migration',\n      default: DEFAULT_MODEL,\n    })\n    .option('maxConcurrency', {\n      type: 'number',\n      default: 25,\n      description:\n        'Maximum number of concurrent requests to the API. Higher numbers may hit usages limits',\n    })\n    .option('temperature', {\n      type: 'number',\n      alias: 't',\n      default: DEFAULT_TEMPERATURE,\n      description: 'Temperature for the model. Lower temperature reduces randomness/creativity',\n    })\n    .option('apiKey', {\n      type: 'string',\n      alias: 'a',\n      description: 'API key used when making calls to the Gemini API',\n    });\n}\n\n/** Yargs command handler for the command. */\nasync function handler(options: Arguments<Options>) {\n  const apiKey = options.apiKey || DEFAULT_API_KEY;\n\n  assert(\n    apiKey,\n    [\n      'No API key configured. A Gemini API key must be set as the `GEMINI_API_KEY` environment ' +\n        'variable, or passed in using the `--api-key` flag.',\n      'For internal users, see go/aistudio-apikey',\n    ].join('\\n'),\n  );\n\n  const [files, prompt] = await Promise.all([\n    glob([options.files]),\n    readFile(options.prompt, 'utf-8'),\n  ]);\n\n  if (files.length === 0) {\n    Log.error(`No files matched the pattern \"${options.files}\"`);\n    process.exit(1);\n  }\n\n  const ai = new GoogleGenAI({apiKey});\n  const progressBar = new SingleBar({}, Presets.shades_grey);\n  const failures: {name: string; error: string}[] = [];\n  const running = new Set<Promise<void>>();\n\n  Log.info(\n    [\n      `Applying prompt from ${options.prompt} to ${files.length} files(s).`,\n      `Using model ${options.model} with a temperature of ${options.temperature}.`,\n      '', // Extra new line at the end.\n    ].join('\\n'),\n  );\n  progressBar.start(files.length, 0);\n\n  // Kicks off the maximum number of concurrent requests and ensures that as many requests as\n  // possible are running at the same time. This is preferrable to chunking, because it allows\n  // the requests to keep running even if there's one which is taking a long time to resolve.\n  while (files.length > 0 || running.size > 0) {\n    // Fill up to maxConcurrency\n    while (files.length > 0 && running.size < options.maxConcurrency) {\n      const file = files.shift()!;\n      const task = processFile(file).finally(() => running.delete(task));\n      running.add(task);\n    }\n\n    // Wait for any task to finish\n    if (running.size > 0) {\n      await Promise.race(running);\n    }\n  }\n\n  progressBar.stop();\n\n  for (const {name, error} of failures) {\n    Log.info('-------------------------------------');\n    Log.info(`${name} failed to migrate:`);\n    Log.info(error);\n  }\n\n  Log.info(`\\nDone ðŸŽ‰`);\n\n  if (failures.length > 0) {\n    Log.info(`${failures.length} file(s) failed. See logs above for more information.`);\n  }\n\n  async function processFile(file: string): Promise<void> {\n    try {\n      const content = await readFile(file, 'utf-8');\n      const result = await applyPrompt(ai, options.model, options.temperature, content, prompt);\n      await writeFile(file, result);\n    } catch (e) {\n      failures.push({name: file, error: (e as Error).toString()});\n    } finally {\n      progressBar.increment();\n    }\n  }\n}\n\n/**\n * Applies a prompt to a specific file's content.\n * @param ai Instance of the GenAI SDK.\n * @param model Model to use for the prompt.\n * @param temperature Temperature for the promp.\n * @param content Content of the file.\n * @param prompt Prompt to be run.\n */\nasync function applyPrompt(\n  ai: GoogleGenAI,\n  model: string,\n  temperature: number,\n  content: string,\n  prompt: string,\n): Promise<string> {\n  // The schema ensures that the API returns a response in the format that we expect.\n  const responseSchema = {\n    type: 'object',\n    properties: {\n      content: {type: 'string', description: 'Changed content of the file'},\n    },\n    required: ['content'],\n    additionalProperties: false,\n    $schema: 'http://json-schema.org/draft-07/schema#',\n  };\n\n  // Note that technically we can batch multiple files into a single `generateContent` call.\n  // We don't do it, because it increases the risk that we'll hit the output token limit which\n  // can corrupt the entire response. This way one file failing won't break the entire run.\n  const response = await ai.models.generateContent({\n    model,\n    contents: [{text: prompt}, {text: content}],\n    config: {\n      responseMimeType: 'application/json',\n      responseSchema,\n      temperature,\n      // We need as many output tokens as we can get.\n      maxOutputTokens: Infinity,\n      // We know that we'll only use one candidate so we can save some processing.\n      candidateCount: 1,\n      // Guide the LLM towards following our schema.\n      systemInstruction:\n        `Return output following the structured output schema. ` +\n        `Return an object containing the new contents of the changed file.`,\n    },\n  });\n\n  const text = response.text;\n\n  if (!text) {\n    throw new Error(`No response from the API. Response:\\n` + JSON.stringify(response, null, 2));\n  }\n\n  let parsed: {content?: string};\n\n  try {\n    parsed = JSON.parse(text) as {content: string};\n  } catch {\n    throw new Error(\n      'Failed to parse result as JSON. This can happen if if maximum output ' +\n        'token size has been reached. Try using a different model. ' +\n        'Response:\\n' +\n        JSON.stringify(response, null, 2),\n    );\n  }\n\n  if (!parsed.content) {\n    throw new Error(\n      'Could not find content in parsed API response. This can indicate a problem ' +\n        'with the request parameters. Parsed response:\\n' +\n        JSON.stringify(parsed, null, 2),\n    );\n  }\n\n  return parsed.content;\n}\n\n/** CLI command module. */\nexport const MigrateModule: CommandModule<{}, Options> = {\n  builder,\n  handler,\n  command: 'migrate',\n  describe: 'Apply a prompt-based AI migration over a set of files',\n};\n"]}